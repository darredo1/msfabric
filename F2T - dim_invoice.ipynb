{"cells":[{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"abfss://960e0a2f-cbbd-4cd0-9b82-386320f9bef1@onelake.dfs.fabric.microsoft.com/918ea8d9-12b6-4811-8394-eda23cd4d582/Files/Alma Analytics/Fund Transactions/DIM - Invoice.csv\")\n","# df now is a Spark DataFrame containing CSV data from \"Files/Alma Analytics/Fund Transactions/DIM - Invoice.csv\".\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"4bc668b6-5ee5-4f90-a126-02d41841b1d0"},{"cell_type":"code","source":["# Code generated by Data Wrangler for PySpark DataFrame\n","\n","from pyspark.sql import types as T\n","\n","def clean_data(df):\n","    # Drop duplicate rows in column: 'Invoice Number'\n","    df = df.dropDuplicates(['Invoice Number'])\n","    # Change column type to datetime64[ns] for column: 'Invoice Payment Voucher Date'\n","    df = df.withColumn('Invoice Payment Voucher Date', df['Invoice Payment Voucher Date'].cast(T.TimestampType()))\n","    # Change column type to float64 for column: 'Invoice Payment Voucher Amount'\n","    df = df.withColumn('Invoice Payment Voucher Amount', df['Invoice Payment Voucher Amount'].cast(T.DoubleType()))\n","    # Replace missing values with \"<na>\" in columns: 'Invoice External Reference Number'\n","    df = df.fillna(value=\"<na>\", subset=['Invoice External Reference Number'])\n","    return df\n","\n","df_clean = clean_data(df)\n","display(df_clean)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"c98f2b98-2ccc-4cd4-8144-843932d30e0b"},{"cell_type":"code","source":["df_clean.write.format(\"delta\").mode('overwrite').option(\"delta.columnMapping.mode\", \"name\").saveAsTable(\"dim_invoice\")\n","\n","df_clean.printSchema()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a225d5e7-cc65-4ed7-853f-c4bf7c7ca43a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"918ea8d9-12b6-4811-8394-eda23cd4d582","known_lakehouses":[{"id":"918ea8d9-12b6-4811-8394-eda23cd4d582"}],"default_lakehouse_name":"LibraryEnterpriseData","default_lakehouse_workspace_id":"960e0a2f-cbbd-4cd0-9b82-386320f9bef1"}}},"nbformat":4,"nbformat_minor":5}